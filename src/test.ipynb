{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_words = 18452\n",
      "Creating dataset...\n",
      "X_train.shape = (107097,)\n",
      "y_train.shape = (107097,)\n",
      "X_test.shape = (26775,)\n",
      "y_test.shape = (26775,)\n",
      "maxlen = 16\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import os\n",
    "\n",
    "ngrams = 3\n",
    "filepath = 'data/wiki_name_race.txt'\n",
    "save_path = 'data'\n",
    "\n",
    "print (\"ngrams = %d\" % ngrams)\n",
    "print (\"filepath = %s\" % filepath)\n",
    "\n",
    "print (\"Loading data...\")\n",
    "# Wikilabels\n",
    "df = pd.read_csv(filepath)\n",
    "df.dropna(subset=['name_first', 'name_last'], inplace=True)\n",
    "# add middle name to first name\n",
    "df['name_first'] = df['name_first'] + ' ' + df['name_middle'].fillna('')\n",
    "# drop middle name column\n",
    "df = df.drop('name_middle', axis=1)\n",
    "# concat last name and first name\n",
    "df['name_last_name_first'] = df['name_last'] + ' ' + df['name_first'] \n",
    "\n",
    "print (\"creating n-gram vocabulary...\") \n",
    "# build n-gram list\n",
    "vect = CountVectorizer(analyzer='char', ngram_range=(ngrams, ngrams), lowercase=False) \n",
    "a = vect.fit_transform(df.name_last_name_first)\n",
    "vocab = vect.vocabulary_ \n",
    "\n",
    "# sort n-gram by freq (highest -> lowest)\n",
    "words = []\n",
    "for b in vocab:\n",
    "    freq = vocab[b]\n",
    "    \n",
    "    words.append((a[:, freq].sum(), b))\n",
    "    #break\n",
    "words = sorted(words, reverse=True)\n",
    "words_list = ['<UNK>']\n",
    "words_list.extend([w[1] for w in words])\n",
    "num_words = len(words_list)\n",
    "print(\"num_words = %d\" % num_words)\n",
    "\n",
    "ngram2idx = {ngram: i for i, ngram in enumerate(words_list)}\n",
    "idx2ngram = {i: ngram for i, ngram in enumerate(words_list)}\n",
    "\n",
    "print (\"Creating dataset...\") \n",
    "# build X from index of n-gram sequence\n",
    "X = np.array(df.name_last_name_first.apply(lambda c: [ngram2idx.get(ngram, 0) for ngram in [c[i:i+ngrams] for i in range(len(c)-ngrams+1)]]))\n",
    "\n",
    "races = np.unique(df.race.astype('category'))\n",
    "race2idx = {x:i for i,x in enumerate(races)}\n",
    "idx2race = {i:x for i,x in enumerate(races)}\n",
    "\n",
    "y = np.array(df.race.apply(lambda c: race2idx[c]))\n",
    "\n",
    "# Split train and test dataset\n",
    "X_train,  X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21, stratify=y)\n",
    "print (\"X_train.shape = %s\" % str(X_train.shape))\n",
    "print (\"y_train.shape = %s\" % str(y_train.shape))\n",
    "print (\"X_test.shape = %s\" % str(X_test.shape))\n",
    "print (\"y_test.shape = %s\" % str(y_test.shape))\n",
    "\n",
    "# padd sequences to the 80th percentile\n",
    "maxlen = np.percentile([len(x) for x in X_train], 80)\n",
    "maxlen = int(maxlen)\n",
    "print(\"maxlen = %d\" % maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataset...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# pad with 0s (i.e. UNK token)\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "X_train = pad_sequences(X_train, maxlen=maxlen, padding='post', truncating='post')\n",
    "X_test = pad_sequences(X_test, maxlen=maxlen, padding='post', truncating='post')\n",
    "\n",
    "print (\"Saving dataset...\")\n",
    "# save dataset \n",
    "save_path = os.path.join(save_path, 'ngrams_%d' % ngrams)\n",
    "# create folder if not exist\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "np.savetxt(os.path.join(save_path, 'X_train.txt'), X_train, fmt='%d')\n",
    "np.savetxt(os.path.join(save_path, 'X_test.txt'), X_test, fmt='%d')\n",
    "np.savetxt(os.path.join(save_path, 'y_train.txt'), y_train, fmt='%d')\n",
    "np.savetxt(os.path.join(save_path, 'y_test.txt'), y_test, fmt='%d')\n",
    "\n",
    "# save vocab\n",
    "with open(os.path.join(save_path, 'vocab.txt'), 'w') as f:\n",
    "    for v, fr in vocab.items():\n",
    "        f.write(f\"{v}\\t{fr}\\n\")\n",
    "\n",
    "# save ngram2idx\n",
    "with open(os.path.join(save_path, 'ngram2idx.txt'), 'w') as f:\n",
    "    for v, fr in ngram2idx.items():\n",
    "        f.write(f\"{v}\\t{fr}\\n\")\n",
    "\n",
    "# save idx2ngram\n",
    "with open(os.path.join(save_path, 'idx2ngram.txt'), 'w') as f:\n",
    "    for v, fr in idx2ngram.items():\n",
    "        f.write(f\"{v}\\t{fr}\\n\")\n",
    "\n",
    "# save race2idx\n",
    "with open(os.path.join(save_path, 'race2idx.txt'), 'w') as f:\n",
    "    for v, fr in race2idx.items():\n",
    "        f.write(f\"{v}\\t{fr}\\n\")\n",
    "\n",
    "# save idx2race\n",
    "with open(os.path.join(save_path, 'idx2race.txt'), 'w') as f:\n",
    "    for v, fr in idx2race.items():\n",
    "        f.write(f\"{v}\\t{fr}\\n\")\n",
    "\n",
    "print (\"Done!\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e47b1a34c05c1e3b83a62d7885c9d1b5ef8a0522d3be0182d0a008ec409b2b3d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
